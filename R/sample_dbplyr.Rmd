---
title: "Example code for using dbplyr arrow package to handle big dataset"
author: "Zhenguo Zhang"
date: "`r Sys.Date()`"
# use github format
output:
  github_document:
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message=FALSE)
workDir<-tempdir()
```

```{r}
library(DBI)
library(dbplyr)
library(tidyverse)
```


This document is based on the page https://r4ds.hadley.nz/databases.html

In R, there are two important packages to handle databases:

- DBI: a common low-level interface between R and database management systems
- dbplyr: a dplyr backend that translates your dplyr code to SQL and then executes
  them with DBI.
  
Databases are run by database management systems (DBMS’s for short), which come in three basic forms:

- Client-server DBMS’s run on a powerful central server, which you connect to from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.
- Cloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.
- In-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user.

## Play the dbduck database

### Create a database

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = file.path(workDir, "duckdb"))
```

### Load some data

We will use the `DBI::dbWriteTable()` function to write some ggplot datasets into tables:

```{r}
dbWriteTable(con, "mpg", ggplot2::mpg)
dbWriteTable(con, "diamonds", ggplot2::diamonds)
```

In practice, one can use `duckdb_read_csv()` and `duckdb_register_arrow()` to quickly load data
into duckdb database without reading them into R. 


### List tables in the database

```{r}
dbListTables(con)
```

### Use dbplyr to interact with the database

To use dbplyr, we first need to use the `tbl()` function from dbplyr to reference a table in the database:

```{r}
mpg_db <- tbl(con, "mpg")
head(mpg_db)
```
 First, many corporate databases are very large so you need some hierarchy to keep all the tables organized. In that case you might need to supply a schema, or a catalog and a schema, in order to pick the table you’re interested in:

```{r, eval=FALSE}
mpg_db <- tbl(con, in_schema("schema_name", "mpg"))
mpg_db <- tbl(con, in_catalog("catalog_name", "schema_name", "mpg"))
```


One can shw the SQL query generated by a dbplyr pipeline using the `show_query()` function:

```{r}
mpg_db %>%
  filter(cty > 20) %>%
  show_query()
```

The dbplyr syntax is lazy, meaning it will not execute unless it is necessary.
To actually execute the query and return the results as a data frame, one can use the `collect()` function:

```{r}
mpg_db %>%
  filter(cty > 20) %>%
  collect()
```

Typically, you’ll use dbplyr to select the data you want from the database, performing basic filtering and aggregation using the translations described below. Then, once you’re ready to analyse the data with functions that are unique to R, you’ll collect() the data to get an in-memory tibble, and continue your work with pure R code.


### Exectute sql queries

```{r}
sql <- "
  SELECT carat, cut, clarity, color, price 
  FROM diamonds 
  WHERE price > 15000
"
dbGetQuery(con, sql) |>
  as_tibble()
```


## Access Arrow object

You can also use dbplyr to access Arrow objects via duckdb. First, let's create an Arrow dataset:

```{r}
library(arrow)
# create a sample arrow dataset
arrow_data_dir <- file.path(workDir, "arrow_data")
dir.create(arrow_data_dir, recursive = TRUE, showWarnings = FALSE)
write_dataset(
  iris,
  path = arrow_data_dir,
  format = "parquet"
)

# read the dataset back into an arrow object and then use to_duckdb() to convert to a duckdb table
iris_arrow <- open_dataset(arrow_data_dir)

iris_arrow |>
  to_duckdb() |>
  group_by(Species) |>
  summarise(Avg_Sepal_Length = mean(Sepal.Length)) |>
  head()
```

To convert a duckdb object into arrow object, one can use the `to_arrow()` function.
The conversion provides benefits that when some features are not supported in either duckdb or arrow, one can switch to the other backend to perform the operation.
For example,  DuckDB cannot yet open multi-level files created by partitioning. 
To do this, you must therefore go through Arrow and open the dataset, and then
use `to_duckdb` for following operations.


Let's finally disconnet from the database:

```{r}
dbDisconnect(con, shutdown = TRUE)
```



## List of useful functions from dbplyr

| Function | Example | Description |
|----------|---------|-------------|
| tbl() | `tbl(con, "mpg")` | Reference a table in the database |
| in_schema() | `tbl(con, in_schema("schema_name", "mpg"))` | Reference a table in a specific schema |
| in_catalog() | `tbl(con, in_catalog("catalog_name", "schema_name", "mpg"))` | Reference a table in a specific catalog and schema |
| show_query() | `mpg_db %>% show_query()` | Show the SQL query generated by a dbplyr pipeline |
| collect() | `mpg_db %>% filter(cty > 20) %>% collect()` | Execute the query and return the results as a data frame |
| left_join() | `tbl1 %>% left_join(tbl2, by = "id")` | Perform a left join between two tables |
| inner_join() | `tbl1 %>% inner_join(tbl2, by = "id")` | Perform an inner join between two tables |
| right_join() | `tbl1 %>% right_join(tbl2, by = "id")` | Perform a right join between two tables |
| full_join() | `tbl1 %>% full_join(tbl2, by = "id")` | Perform a full join between two tables |
| pivot_wider() | `tbl %>% pivot_wider(names_from = "key", values_from = "value")` | Pivot a table from long to wide format |
| pivot_longer() | `tbl %>% pivot_longer(cols = c("col1", "col2"), names_to = "key", values_to = "value")` | Pivot a table from wide to long format |
| case_when() | `tbl %>% mutate(new_col = case_when(condition1 ~ value1, condition2 ~ value2))` | Create a new column based on multiple conditions |
| if_else() | `tbl %>% mutate(new_col = if_else(condition, true_value, false_value))` | Create a new column based on a single condition |



## List of useful functions from DBI

| Function                  | Description                                      |
|---------------------------|--------------------------------------------------|
| dbConnect()               | Connect to a database                            |
| dbDisconnect()            | Disconnect from a database                       |
| dbListTables()           | List all tables in the database                  |
| dbListFields()           | List all fields (columns) in a table             |
| dbReadTable()            | Read an entire table into R as a data.frame      |
| dbWriteTable()           | Write an R data frame to a table in the database |
| dbGetQuery()             | Execute a SQL query and return the result as a data frame |
| dbExecute()              | Execute a SQL statement without returning a result |
| dbRemoveTable()          | Remove a table from the database                 |
| dbExistsTable()          | Check if a table exists in the database          |
| dbGetInfo()              | Get information about the database connection    |
| dbBegin(), dbCommit(), dbRollback() | Manage transactions (begin, commit, rollback) |
| dbSendQuery(), dbFetch(), dbClearResult() | Send a query, fetch results, and clear the result set |



## References

- dm: an R package to bridge the gap in the data pipeline between individual data frames and relational databases https://dm.cynkra.com/
- Manipulate big data with Arrow and Duckdb: https://www.christophenicault.com/post/large_dataframe_arrow_duckdb/

